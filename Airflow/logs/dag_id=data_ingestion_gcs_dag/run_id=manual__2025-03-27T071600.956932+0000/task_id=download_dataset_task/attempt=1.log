[2025-03-27T07:16:04.356+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task manual__2025-03-27T07:16:00.956932+00:00 [queued]>
[2025-03-27T07:16:04.372+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task manual__2025-03-27T07:16:00.956932+00:00 [queued]>
[2025-03-27T07:16:04.373+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2025-03-27T07:16:04.375+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 2
[2025-03-27T07:16:04.375+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2025-03-27T07:16:04.395+0000] {taskinstance.py:1383} INFO - Executing <Task(BashOperator): download_dataset_task> on 2025-03-27 07:16:00.956932+00:00
[2025-03-27T07:16:04.411+0000] {standard_task_runner.py:54} INFO - Started process 1716 to run task
[2025-03-27T07:16:04.417+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'download_dataset_task', 'manual__2025-03-27T07:16:00.956932+00:00', '--job-id', '28', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpbpe789fm']
[2025-03-27T07:16:04.421+0000] {standard_task_runner.py:83} INFO - Job 28: Subtask download_dataset_task
[2025-03-27T07:16:04.423+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/data_ingestion_gcs_dag.py
[2025-03-27T07:16:04.515+0000] {dagbag.py:330} ERROR - Failed to import: /opt/***/dags/data_ingestion_gcs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 326, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 73, in <module>
    create_empty_dataset_task = BigQueryCreateEmptyDatasetOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 408, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 1346, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 408, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 755, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to BigQueryCreateEmptyDatasetOperator (task_id: create_empty_dataset_task). Invalid arguments were:
**kwargs: {'datasetid': 'tate_data_all'}
[2025-03-27T07:16:04.551+0000] {standard_task_runner.py:102} ERROR - Failed to execute job 28 for task download_dataset_task (Dag 'data_ingestion_gcs_dag' could not be found; either it does not exist or it failed to parse.; 1716)
[2025-03-27T07:16:04.588+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2025-03-27T07:16:04.641+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
